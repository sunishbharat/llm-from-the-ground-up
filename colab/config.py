#
# Transformer configuration attributes are defined here.
#

GPT_CONFIG_124M = {
    "num_heads"   : 2,
    "embd_dim"    : 768,
    "context_len" : 1024,
    "dropout_rate": 0.,
    "qkv_bias"    : False
}



