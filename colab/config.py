#
# Transformer configuration attributes are defined here.
#

GPT_CONFIG_124M = {
    "vocab_size"  : 50257,
    "context_len" : 7,#1024,
    "embd_dim"    : 768,
    "num_heads"   : 12,
    "num_layers"  : 12,
    "dropout_rate": 0.,
    "qkv_bias"    : False,
}



